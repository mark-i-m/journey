\documentclass[twocolumn,11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{caption}

\title{Journey: A Study of Studying Large Memory Systems}
\author{Mark Mansi, Suhas Pai, Hasnain Ali Pirzada}
\date{}

\begin{document}

\maketitle

\section{Introduction}

Recent advances in memory technologies are making it possible for machines with
large amounts of memory to become commonplace. Many current systems already
have dozens or hundreds of gigabytes of memory. These technology advances can
enable systems with terabytes of memory in the not very distant future. For
example, non-volatile memory technologies, which have denser physical
structures than traditional DRAM are making such large memories available and
more accessible \cite{xpoint}.

Much research is currently being done to study how non-volatile and large
memory systems would change system design. HP has been working on a system
which it calls ``The Machine'', in which hundreds of cores have access to a
multi-petabyte shared memory pool \cite{hp_machine}. Intel and
Micron have announced their 3D Xpoint non-volatile memory technology, making
dense, fast, persistent memory comercially available \cite{xpoint}.

However, studies of large-memory systems are often forced to use simulation or
emulation to evaluate solutions because most researchers do not have access to
these sorts of systems for experimentation \cite{quartz}. This often
forces researchers to simplify models, incurring inaccuracy, or to use smaller
benchmarks, which are less realistic. We note that there was a similar problem
when large storage systems first became available \cite{david, exalt}.

Currently, there are few good tools available to systems researchers for
understanding how systems interact with large memories. The ideal end-goal of
this research is to identify bottlenecks in managing large memories and find a
general lightweight technique or tool for studying and designing operating
systems capable of handling large memory while using systems currently
available to researchers.

(TODO: we might want to add stuff about using swap instead of larger memory)

\section{Related Work}

The emergence of new technology has in the past often required operating systems
to change. Traditionally, OS researchers and developers benchmark and test their
solutions in new environments to demonstrate their efficacy before using them in
real systems. However, researchers do not always have access to new technology
for practical reasons; for example, large memory systems are still too expensive
for most research budgets. This is not the first time that researchers have been
forced to study systems in environments they do not have access to. This section
examines past approaches to studying these systems and motivates the need for
our work.

The emergence of the internet required businesses to buy expensive machines to
keep up with demand for their services, but systems researchers did not have
access to these machines. Alameldeen et al. describe how they simulate a
multi-million dollar server using a \$2000 workstation. To do this, they go
through several rounds of scaling down, optimizing, changing, and tuning both
the benchmarks and systems they test. For example, they scale down a workload to
fit in the 1GB memory of the machine they use and increase the number of threads
to improve parallelism \cite{2kmachine}. This methodology may be accurate, but
it requires modifying the benchmarks, which is error prone and time consuming;
it can be extremely difficult to validate changes to a benchmark. In our work,
we seek to avoid changes to the experimental workloads including changes to code
or scale.

Likewise, the Quartz emulator studies the behaviour of applications on
Non-volatile Memory (NVM) while actually running on top of DRAM. It uses
existing hardware capabilities to emulate the higher latency and lower bandwidth
of most NVM technologies. While Quartz does not address the fact that the actual
memory available to the applications on NVM might be orders of magnitude higher
than on DRAM, it demonstrates that new technologies may be emulated using
existing technology \cite{quartz}.

The Simics simulator uses demand paging for simulated environments, which means
that memory is only allocated when it is used. As long as the working set of the
target system fits in the host memory, performance will be tolerable.  However,
in our work, we wish to study OS behavior under workloads that actually use all
available memory. Thus, the usefulness of demand paging is greatly reduced. For
these workloads, the performance of simulators like Simics tends to be
impractical \cite{simics}. We do not explore simulation further because it can
be orders of magnitude slower than native execution, making it infeasible for
studying large workloads or systems \cite{2kmachine}.

A similar problem was encountered by researchers studying both large storage
drives and large distributed storage systems. David is a system that allows
storage and big data researchers to run large benchmarks requiring terabytes of
storage using off the shelf storage devices (which at that time were too small).
David creates a compressed version of the file system by physically storing only
metadata and discarding the contents of files. Reads to the disk causes David to
generate data on the fly. This decision choice is based on the observation that
most benchmarking frameworks do not care about the actual content of the files,
and that most of the storage capacity of a drive tends to be data rather than
metadata \cite{david}. The Exalt system uses a similar methodology for
large-scale distributed storage systems \cite{exalt}. This methodology provides
a promising direction for large-memory system studies. One may consider ignoring
the contents of a process’s heap and only storing kernel data structures and a
process’s code and stack segments. However, generating heap contents on the fly
is more difficult than generating disk contents because of the common use of
custom data structures. Moreover, on Linux, the kernel data structures for 1TB
of memory may also exceed the size of physical memory on present systems
\cite{simics}.

A virtualized environment can be used to provide a guest system with more memory
than is physically available to the host. A study of the limits of the KVM
hypervisor found that there is no fundamental limit to the size of guest
physical memory other than the hardware address width. However, currently, a
Linux host running KVM will require the guest memory to be backed by host
physical memory or host swap space \cite{ibmkvm}.  This means that when the
virtual machine uses the whole amount of memory allocated to it, the host can
swap pages to disk. The resulting poor performance can cause inaccurate
performance measurements when running benchmarks.

Prefetching pages from swap space can offer a way to mitigate the overhead of
memory overcommitment. When there is significant memory pressure, even pages
which are likely to be accessed soon are swapped out to disk. They are faulted
back into memory when accessed, resulting in significant performance cost.
Charm++ uses a programming model where computations can be scheduled by the
language runtime. A designated thread can prefetch pages required by a
computation, averting page faults. Although this approach is effective in
avoiding overhead, it requires applications to be written with a particular
programming language and so is not applicable to most benchmarks without
changing them \cite{charmpp}.

Gupta et al. built an emulator for high speed networks using a technique called
time dilation, which slows down the OS clock to make it appear that external
events are occurring faster. This allows the system to emulate network links
with speeds that are currently not available. The implementation is based on the
VMs and Xen hypervisor; Xen delivers the timer interrupts to the guest at a
lower rate than hardware hence  slowing down the guest’s clock \cite{timedil}.
We may use a similar approach to slow down system time  while paging in and out
large portions of memory. This will allow us to make the system believe that it
is reading data from the memory while actually most of that data is being read
from the disk.

Finally, there have been studies which look at the performance overheads
associated with current implementations of virtual memory and suggest mechanisms
to mitigate them. RadixVM tries to overcome performance issues in highly
concurrent workloads due to serialization of memory management operations on
kernel data structures \cite{radixvm}. This work demonstrates that many parts of
existing memory management schemes are not scalable to larger systems.
Similarly, in our work, we wish to examine scalability limitations of memory
management in the Linux kernel. Other studies have suggested that \texttt{struct
page}, \texttt{struct vm\_area\_struct}, and page tables tend to comprise a
large portion of memory management overhead \cite{simics}.

\section{Methodology}

Our methodology is broken into two parts. First, we measure the scalability of
Linux's \texttt{vm\_area\_struct}. Then, we measure the minimum amount of memory
necessary for swapping.

\subsection{\texttt{vm\_area\_struct} Experiments}

\begin{figure}
\centering
\begin{tabular}{|l|p{5cm}|} \hline
OS & Ubuntu 16.04.2 \\ \hline
Kernel & Linux 4.4.0-70 \\ \hline
CPU & Intel Xeon E5645, 2.40 GHz, 6 cores/12 threads, 384KB Private L1-I Cache,
1.536 Unified L2 Cache, 12MB Shared L3 Cache \\ \hline
Memory & 16GB, 1066MHz \\ \hline
Storage & 500GB, 7200RPM \\
\hline
\end{tabular}
\caption{Specifications of our test machine.  \label{fig:specs}}
\end{figure}

We measure the memory overhead and latency of the Linux Memory Management
system's use of \texttt{vm\_area\_struct}, which represents a region of a
process's virtual address space.  We do not measure the overhead of
\texttt{struct page} because it is known that there is one struct for each
physical page. We do not measure the overhead of page tables because this can be
calculated from the size of the virtual memory space used by the process.  When
a program makes a memory management system calls such as \texttt{mmap},
\texttt{mremap}, \texttt{munmap}, \texttt{mprotect}, \texttt{mlock}, or
\texttt{madvise}, the kernel is creating, changing, or removing
\texttt{vm\_area\_struct}s; thus, measuring the scalability of Linux memory
management requires measuring the overhead of these structs.

We run several different microbenchmarks, each of which has a different memory
allocation pattern. These benchmarks are designed to stress the memory
management system in different ways, rather than simulate real workloads. For
each workload, we measure the latency and increase in memory usage due to each
operation in the benchmark. Notably, we do not touch the pages that we allocate,
avoiding both a page fault and the allocation of a back physical page for the
allocated virtual pages. This allows us to measure just the overhead of memory
management, as opposed to physical page allocation, swapping, page faults, or
other overheads.  Each workload contains $2^{20}$ memory management operations.

The specifications of the machine are listed in Figure \ref{fig:specs}. We
disabled Intel SpeedStep and TurboBoost. All experiments are pinned to the same
core on our machine. Each benchmark was run 5 times and the results aggregated
as described below. We reboot the machine when switching to a different
benchmark, but keep the machine running continually during the runs of the same
benchmark. Each run is executed in isolation, without any additional processes
running beyond services that start when the system is booted and a single ssh
session. We are careful to avoid opening extra ssh sessions, running screen or
tmux, or executing extra commands while a benchmark is running, as these may
impact memory usage, and thus our results.

We measured latency using the \texttt{rdtsc} instruction provided by x86\_64
processors, which gives high resolution cycle-level timestamps. We take a
timestamp before and after each memory management operation in the workload and
take the difference to get latency. The median latency of each operation is
reported across all 5 runs.

To measure memory, we use the procfs's reporting on current memory usage, along
with the slab allocator's count of active \texttt{vm\_area\_struct}s in the
system. Measuring kernel memory usage and associating memory usage with
processes is very difficult. Our methodology assumes that our benchmarks running
in otherwise idle systems should dominate increases in memory usage.  Thus, we
compute total memory usage in the system as the total amount of memory less the
amount of memory used. From this amount, we subtract the resident set size of
the benchmark process. The result is the memory usage of the rest of the system
$R$. We denote $R_i$ as the $i$-th measurement of $R$ during a run ($i = 0, 1,
... 2^{20}$). System memory usage is known to jitter in Linux (TODO: cite). To
adjust for this jitter, we define $\Delta_i = R_i - R_0$. For each operation, we
take the median value of $\Delta_i$ across the 5 runs to reduce jitter further.
This is the value graphed in the figures in this section. $\Delta_i$ represents
the increase in memory usage due to memory management overheads caused by
running $i$ memory management operations.

The workloads we measure are

\begin{itemize} \item \textit{Control}. This workload is only run for the memory
overhead experiments. It simply prints the $\Delta_i$ for each $i$. It's graph
is a straight line with positive slope. This memory usage results from the
buffer containing stdout, where our readings are being printed. All of our
experiments have the same stdout overhead, so this benchmark serves a baseline
for comparison.

\item \textit{Continuous} (``cont'' for short). This workload allocates $2^{20}$
adjacent 16KB blocks.

\item \textit{Strided}. This workload allocates $2^{20}$ untouching 16KB regions
in order of increasing memory address.

\item \textit{Random}. This workload allocates randomly the pages in a
inside of a $2^{20}$ page region.

\item \textit{Fragmented} (``frag'' for short). This workload first allocates
$2^{19} + 2^{18}$ contiguous 4KB blocks. Then, it resizes every other block to
40KB. This represents the worst case behavior we could think of. It requires the
kernel to find a block it has not touched in over $2^{19}$ operations and move
it somewhere else.  \end{itemize}

\subsection{Swapping Experiments}

These experiments are designed to measure the minimum amount of memory needed
to using swapping.

In this experiment, a single process touches 32GB of virtual memory, leading to
the actual allocation of physical memory. Notice that 32GB is twice as large as
the 16GB of physical memory the test machine. This causes Linux kernel to start
swapping out pages to make more room.

After each page of virtual memory is touched, the resident set size (RSS; the
amount of physical memory Linux has allocated to the process) is recorded. This
value is graphed in our results section.

We repeat this experiment with 5 different swapfile sizes to capture the impact
of swapfile size on swapping overhead.

(TODO: strided access vs random access?)

\section{Result}

\subsection{\texttt{vm\_area\_struct} Experiments}

TODO

\subsection{Swapping Experiments}

TODO

\bibliography{references}{}
\bibliographystyle{plain}

\end{document}
