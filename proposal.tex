\documentclass[twocolumn,11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{caption}

\title{Journey: A Study of Studying Large Memory Systems}
\author{Mark Mansi, Suhas Pai, Hasnain Ali Pirzada}
\date{}

\begin{document}

\maketitle

\section{Motivation}

Recent advances in memory technologies are making it possible for machines with
large amounts of memory to become commonplace. Many current systems already
have dozens or hundreds of gigabytes of memory. These technology advances can
enable systems with terabytes of memory in the not very distant future. For
example, non-volatile memory technologies, which have denser physical
structures than traditional DRAM are making such large memories available and
more accessible \cite{xpoint}.

Much research is currently being done to study how non-volatile and large
memory systems would change system design. HP has been working on a system
which it calls ``The Machine'', in which hundreds of cores have access to a
multi-petabyte shared memory pool \cite{hp_machine}. Intel and
Micron have announced their 3D Xpoint non-volatile memory technology, making
dense, fast, persistent memory comercially available \cite{xpoint}.

However, studies of large-memory systems are often forced to use simulation or
emulation to evaluate solutions because most researchers do not have access to
these sorts of systems for experimentation \cite{quartz}. This often
forces researchers to simplify models, incurring inaccuracy, or to use smaller
benchmarks, which are less realistic. We note that there was a similar problem
when large storage systems first became available \cite{david, exalt}.

Currently, there are few good tools available to systems researchers for
understanding how systems interact with large memories. The ideal end-goal of
this research is to identify bottlenecks in managing large memories and find a
general lightweight technique or tool for studying and designing operating
systems capable of handling large memory while using systems currently
available to researchers.

\section{Proposal}

This research will proceed in two phases. First, we will analyze and understand
the existing memory management mechanisms in Linux, particularly the overhead
of its data structures and operations. We will explore the limits of
virtualization techniques with large guest memory and the ways in which the
Linux kernel manages memory, exploring the scalability of Linux memory
management. Specifically, we want to look at cases where the guest has many
times more memory than the host. Part of this phase will involve experimentally
exploring a variety of workloads with large memory footprints.

In the second phase of the research our goal is to build a tool for studying
large-memory systems using more affordable common-sized-memory systems, such as
those to which a typical researcher would have access. Specifically, we will
target operating systems research. We will likely start by implementing a
less-general tool which targets one simple application or a class of
applications. Then, we will make it more general as time permits. The design
choices in our second phase will be heavily influenced by what we find in the
first phase when we study the overhead of large memory systems, so at present
our notions of what we will do are rather vague.


\section{Evaluation}

In the course of our analysis phase, we will identify a number of workloads targeted at large-memory systems. We can use these workloads again to evaluate our solution. Some workloads that we are currently considering are GUPS and memcached.


\section{Required Resources}

We require a machine with enough storage space and memory to develop and test large-memory system emulation techniques.

We will also need access to benchmarks such as GUPS and memcached for our experiments and evaluation.

Finally, we request a large vat of M\&Mâ€™s or Snickers, and a supply of free food for three persons for around three months as a catalyst for our research.

\bibliography{references}{}
\bibliographystyle{plain}

\end{document}
